{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python39064bit7fd64d56249f462394dc94080af000bd",
   "display_name": "Python 3.9.0 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Continuous IBDM\n",
    "\n",
    "This notebook shows how to do image based data mining against a continuous outcome. The outcome could be whatever you like, provided it is a continuous variable; some examples include weight loss, muscle area loss and feeding tube duration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libraries and set up\n",
    "\n",
    "import os\n",
    "import time\n",
    "import os.path\n",
    "import numpy as np\n",
    "try:\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "    haveTQDM = True\n",
    "except:\n",
    "    haveTQDM = False\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "## Make the notebook use full width of display\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n"
   ]
  },
  {
   "source": [
    "We will also download the HNSCC data and unzip it\n",
    "\n",
    "# NOTE: you might not need to do this is you end up on the same runtime as in the registration example, check the file browser on the left"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/msb4uiq8rw79rqe/HNSCC_data.zip?dl=0\n",
    "!unzip HNSCC_data.zip\n",
    "!rm HNSCC_data.zip"
   ]
  },
  {
   "source": [
    "# Getting started\n",
    "\n",
    "I'm going to assume you've already run the binary IBDM notebook, and are familiar with what's going on there. As a result, we will skip straight to loading the same patients as we had in the binary mining, by looking at and filtering the clinical data. I any of this doesn't make sense, refer back to the previous notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinicalDataPath = \"/content/HNSCC_data/clinicalData.csv\"\n",
    "\n",
    "clinicalData = pd.read_csv(clinicalDataPath)\n",
    "\n",
    "ccrtOnlyPatients = clinicalData[(clinicalData[\"Oncologic Treatment Summary\"].str.contains('^CCRT', regex=True)) & (clinicalData[\"Oncologic Treatment Summary\"].str.contains('\\+', regex=True) == False)]\n",
    "len(ccrtOnlyPatients[\"Oncologic Treatment Summary\"])\n",
    "\n",
    "selectedPatients = ccrtOnlyPatients[ccrtOnlyPatients[\"Number of Fractions\"].astype(int) < 40]\n",
    "len(selectedPatients[\"Number of Fractions\"])\n",
    "\n",
    "def calculateVoxelwiseBED(dose, nFrac, alpha_beta=10.0):\n",
    "    factor = 1.0 / (nFrac*alpha_beta)\n",
    "\n",
    "    BED = dose*(1.0 + dose*factor)\n",
    "\n",
    "    return BED\n",
    "\n",
    "\n",
    "dosesPath = \"/content/HNSCC_data/warpedDoses/\"\n",
    "availableDoses = [\"HNSCC-01-{0}\".format(a.split('.')[0]) for a in os.listdir(dosesPath)]\n",
    "\n",
    "availablePatientsMask = selectedPatients['ID'].isin(availableDoses)\n",
    "probeDose = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(dosesPath, \"{0:04d}.nii\".format(int(2)))))\n",
    "\n",
    "selectedPatients = selectedPatients.loc[availablePatientsMask]\n",
    "\n",
    "doseArray = np.zeros((len(selectedPatients), *probeDose.shape))\n",
    "statusArray = np.zeros((len(selectedPatients),))\n",
    "\n",
    "n = 0\n",
    "for idx, pt in selectedPatients.iterrows():\n",
    "    dose_arr = sitk.GetArrayFromImage(sitk.ReadImage(os.path.join(dosesPath, f\"{pt.ID.split('-')[-1]}.nii\" ) ) )\n",
    "    doseArray[n,...] = calculateVoxelwiseBED(dose_arr, pt[\"Number of Fractions\"], alpha_beta=10.0)\n",
    "    n += 1"
   ]
  },
  {
   "source": [
    "Now we have our data, and we have corrected all the doses tot eh same BED, we are ready to do continuous outcome image based data mining.\n",
    "\n",
    "For this we need to select a suitable outcome variable - I suggest weight loss as a good one to start with. \n",
    "\n",
    "The next cell defines a function that calculates the pearson correlation coefficient in each voxel of the dose distribution. To do this, we slightly modify the online calculation of variance used in the binary data mining to do online calculation of covariance. The formula for pearson's correlation coefficient is then:\n",
    "\n",
    "$ \\rho = \\frac{cov(X,Y)}{\\sigma_{x} \\sigma_{y}} $\n",
    "\n",
    "\n",
    "(note: strictly, this is for a population, we have a sample, but the estimates for variance and covariance we return are for a sample, so it will work)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateRho(doseData, continuousOutcome, mask=None):\n",
    "    \"\"\"\n",
    "    Calculate a per-voxel correlation coefficient between two images. Uses Welford's method to calculate mean, variance and covariance. \n",
    "    \n",
    "    Inputs:\n",
    "        - doseData: the dose data, should be structured such that the number of patients in it is along the last axis\n",
    "        - statuses: the outcome labels. 1 indicates an event, 0 indicates no event\n",
    "    Returns:\n",
    "        - rhoValues: an array of the same size as one of the images which contains the per-voxel rho values\n",
    "    \"\"\"\n",
    "    doseMean = np.zeros_like(doseData[...,0])\n",
    "    doseStd = np.zeros_like(doseData[...,0])\n",
    "    covariance = np.zeros_like(doseData[...,0])\n",
    "    C = np.zeros_like(doseData[...,0])\n",
    "    rho = np.zeros_like(doseData[...,0])\n",
    "    \n",
    "    \n",
    "    outcomeMean = 0.0\n",
    "    outcomeVar = 0.0\n",
    "    doseMean[np.where(mask)] += doseData[...,0][np.where(mask)]\n",
    "    outcomeMean += continuousOutcome[0]\n",
    "    subjectCount = 1.0\n",
    "    \n",
    "    for n,y in zip(range(1, doseData.shape[-1]), continuousOutcome[1:]):\n",
    "        x = doseData[...,n]\n",
    "        subjectCount += 1.0\n",
    "        dx = x - doseMean\n",
    "        \n",
    "        om = doseMean.copy()\n",
    "        yom = outcomeMean.copy()\n",
    "\n",
    "        doseMean[np.where(mask)] += dx[np.where(mask)]/subjectCount\n",
    "        outcomeMean += (y - outcomeMean)/subjectCount\n",
    "\n",
    "        doseStd[np.where(mask)] += ((x[np.where(mask)] - om[np.where(mask)])*(x[np.where(mask)] - doseMean[np.where(mask)]))\n",
    "        outcomeVar += (y - yom)*(y - outcomeMean)\n",
    "\n",
    "        C[np.where(mask)] += (dx[np.where(mask)] * (y - outcomeMean))\n",
    "        \n",
    "    doseStd[np.where(mask)] /= (subjectCount)\n",
    "    outcomeVar /= (subjectCount)\n",
    "    covariance[np.where(mask)] = C[np.where(mask)] / (subjectCount - 1) ## Bessel's correction for a sample\n",
    "\n",
    "    rho[np.where(mask)] = covariance[np.where(mask)] / (np.sqrt(doseStd[np.where(mask)]) * np.sqrt(outcomeVar))\n",
    "    return rho"
   ]
  },
  {
   "source": [
    "Now we can apply the mining to some data.  The first thing we need to do is select a continuous outcome variable; we will try weight loss first - we need to create this variable from our database by taking the difference between the starting and ending weight of the patient."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedPatients = selectedPatients.assign(WeightLoss = lambda d : d[\"BW Start tx (kg)\"] - d[\"BW stop treat (kg)\"])\n",
    "weightLoss = selectedPatients.WeightLoss.values "
   ]
  },
  {
   "source": [
    "Now we can do the actual correlation analysis!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = sitk.GetArrayFromImage(sitk.ReadImage(\"/content/HNSCC_data/0002_mask.nii\")).astype(np.int16)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "rhoMap = calculateRho(doseArray, weightLoss, mask=mask)\n",
    "print(time.time() - start)\n",
    "\n",
    "referenceAnatomy = sitk.GetArrayFromImage(sitk.ReadImage(\"/content/HNSCC_data/downsampledCTs/0002.nii\"))\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "anatomy = plt.imshow(referenceAnatomy[::-1,64,...], cmap='Greys_r')\n",
    "rhoMapOverlay = plt.imshow(rhoMap[::-1,64,...], alpha=0.5)\n",
    "_ = plt.axis('off')"
   ]
  },
  {
   "source": [
    "Now we've got our correlation map with the true correspondence of weight loss to dose, we can compute the permutation distribution again to get the significance of the correlation.\n",
    "\n",
    "This works just like before, we just rearrange the weight loss values and re-calculate the correlation coefficient and look at how the most extreme voxels behave"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doPermutation(doseData, outcome, mask=None):\n",
    "    \"\"\"\n",
    "    Permute the statuses and return the maximum t value for this permutation\n",
    "    Inputs:\n",
    "        - doseData: the dose data, should be structured such that the number of patients in it is along the last axis\n",
    "        - outcome: the outcome values. Shuold be a continuous number. These will be permuted in this function to \n",
    "                    assess the null hypothesis of no dose interaction\n",
    "        - mask: A mask outside which we will ignore the returned correlation\n",
    "    Returns:\n",
    "        - (tMin, tMax): the extreme values of the whole t-value map for this permutation\n",
    "    \"\"\"\n",
    "    poutcome = np.random.permutation(outcome)\n",
    "    permT = calculateRho(doseData, poutcome, mask)\n",
    "    return (np.min(permT), np.max(permT))\n",
    "\n",
    "\n",
    "def permutationTest(doseData, outcome, nperm=1000, mask=None):\n",
    "    \"\"\"\n",
    "    Perform a permutation test to get the global p-value and t-thresholds\n",
    "    Inputs:\n",
    "        - doseData: the dose data, should be structured such that the number of patients in it is along the last axis\n",
    "        - outcome: the outcome labels. Should be a continuous number.\n",
    "        - nperm: The number of permutations to calculate. Defaults to 1000 which is the minimum for reasonable accuracy\n",
    "        - mask: A mask outside which we will ignore the returned correlation\n",
    "    Returns:\n",
    "        - globalPNeg: the global significance of the test for negative t-values\n",
    "        - globalPPos: the global significance of the test for positive t-values\n",
    "        - tThreshNeg: the list of minT from all the permutations, use it to set a significance threshold.\n",
    "        - tThreshPos: the list of maxT from all the permutations, use it to set a significance threshold.\n",
    "    \"\"\"\n",
    "    tthresh = []\n",
    "    gtCount = 1\n",
    "    ltCount = 1\n",
    "    trueT = calculateRho(doseData, outcome, mask=mask)\n",
    "    trueMaxT = np.max(trueT)\n",
    "    trueMinT = np.min(trueT)\n",
    "    if haveTQDM:\n",
    "        for perm in tqdm(range(nperm)):\n",
    "            tthresh.append(doPermutation(doseData, outcome, mask))\n",
    "            if tthresh[-1][1] > trueMaxT:\n",
    "                gtCount += 1.0\n",
    "            if tthresh[-1][0] < trueMinT:\n",
    "                ltCount += 1.0\n",
    "    else:\n",
    "        for perm in range(nperm):\n",
    "            tthresh.append(doPermutation(doseData, outcome, mask))\n",
    "            if tthresh[-1][1] > trueMaxT:\n",
    "                gtCount += 1.0\n",
    "            if tthresh[-1][0] < trueMinT:\n",
    "                ltCount += 1.0\n",
    "    \n",
    "    globalpPos = gtCount / float(nperm)\n",
    "    globalpNeg = ltCount / float(nperm)\n",
    "    tthresh = np.array(tthresh)\n",
    "    return (globalpNeg, globalpPos, sorted(tthresh[:,0]), sorted(tthresh[:,1]))"
   ]
  },
  {
   "source": [
    "If we run this function with our data, we will get back the global significance and threshold values of $\\rho$. We can then use a contour plot at the 95th percentile to indicate regions of significance.\n",
    "\n",
    "Let's try doing it now - this is once again equivalent to the binary data mining, but with the continuous data mining code we wrote above. The function call is identical to the binary version, but because the content of the functions is different, it is now doing the calculation with continuous IBDM.\n",
    "\n",
    "*Warning: this cell will take a really long time to run! On my machine, it was about 13 minutes*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pNeg, pPos, threshNeg, threshPos = permutationTest(doseArray, weightLoss, nperm=100, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pNeg, pPos)\n",
    "print(np.percentile(threshNeg, 10))\n",
    "print(threshNeg)\n",
    "print(np.min(rhoMap))\n",
    "\n",
    "print(threshPos)\n",
    "print(np.max(rhoMap))"
   ]
  },
  {
   "source": [
    "The usual threshold for saying a result is statstically sgnificant is p=<0.05. Unfortunately, in my example analysis we don't seem to have a globally significant result. Everything below here won't really work properly because there is not significant result in this case, however let's do it anyway so you can see what to do when you mine something else later and get a significant result!\n",
    "\n",
    "---\n",
    "\n",
    "We also have our map of rho values, and the associated permutation test distribution, so we can plot the regions of significance overlaid on the rho-map and CT anatomy. To do this, we use matplotlib's imshow and contourf functions as below"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# First show the CT\n",
    "ctImg = ax.imshow(referenceAnatomy[:,:,64], cmap='Greys_r')\n",
    "\n",
    "# Now add the t-map with some transparency\n",
    "rhomapImg = ax.imshow(rhoMap[:,:,64], alpha=0.3)\n",
    "\n",
    "plt.axis('off');\n",
    "\n",
    "neg_p005 = np.percentile(threshNeg, 0.05)\n",
    "neg_p010 = np.percentile(threshNeg, 0.10)\n",
    "neg_p015 = np.percentile(threshNeg, 0.15) ## Contour plot needs two levels, so we use p=0.05 & 0.10\n",
    "pos_p005 = np.percentile(threshPos, 0.95)\n",
    "pos_p010 = np.percentile(threshPos, 0.9)\n",
    "pos_p015 = np.percentile(threshPos, 0.75)\n",
    "\n",
    "## Now do the contourplot at the 95% level for p=0.05\n",
    "pos_contourplot = plt.contour(rhoMap[:,:,64], levels=[pos_p015, pos_p010, pos_p005], colors='r')\n",
    "# neg_contourplot = plt.contour(rhoMap[:,:,64], levels=[neg_p005, neg_p010, neg_p015], colors='g')"
   ]
  },
  {
   "source": [
    "Now we have a complete pipeline to do image based data mining!\n",
    "\n",
    "Now use this pipeline to try mining against some of the other outcomes in the clinical data, for example:\n",
    "\n",
    "- Change in BMI between pre/post treatment\n",
    "- Change in skeletal muscle pre/post treatment\n",
    "- Change in other areas, e.g. fats\n",
    "- Skeletal muscle change against CT image density\n",
    "\n",
    "Have fun!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}